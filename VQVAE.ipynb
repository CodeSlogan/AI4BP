{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-27T14:13:24.141601Z",
     "start_time": "2024-06-27T12:17:55.246593Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from data_process.DataModule import DataModule\n",
    "from model.vqvae import VQVAE\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_scaler, output_scaler, train_dataloader, test_dataloader = DataModule()\n",
    "\n",
    "# 参数定义\n",
    "input_channels = 1        # For example, 1 for univariate time series data\n",
    "conv_out_channels = 128    # Number of output channels for the convolution layer\n",
    "kernel_size = 3           # Size of the convolution kernel\n",
    "hidden_size = 256         # Hidden size for LSTM\n",
    "num_layers = 2            # Number of LSTM layers\n",
    "output_size = 1024        # Output size (e.g., length of the output vector)\n",
    "seq_length = 1024         # Length of the input sequence\n",
    "n_embeddings = 512\n",
    "embedding_dim = 64\n",
    "beta = 0.25\n",
    "\n",
    "# Initialize model\n",
    "model = VQVAE(input_channels, conv_out_channels, kernel_size, hidden_size, num_layers, output_size, \n",
    "                 n_embeddings, embedding_dim, beta).to(device)\n",
    "\n",
    "\n",
    "# 超参数\n",
    "num_epochs = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_inputs, batch_targets in train_dataloader:\n",
    "        # 将输入数据移动到设备\n",
    "        batch = batch_inputs.unsqueeze(1).to(device)  # 添加通道维度\n",
    "\n",
    "        # 前向传播\n",
    "        embedding_loss, outputs, perplexity = model(batch)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_targets) + embedding_loss\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "model.eval()  # 设置模型为评估模式\n",
    "total_loss = 0\n",
    "with torch.no_grad():  # 在评估过程中不需要计算梯度\n",
    "    for batch_inputs, batch_targets in test_dataloader:\n",
    "        # 将输入数据和目标输出移动到设备\n",
    "        batch_inputs = batch_inputs.unsqueeze(1).to(device)  # 添加通道维度\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        embedding_loss, outputs, perplexity = model(batch_inputs)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "print(f\"average_loss: {average_loss:.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 0.0525\n",
      "Epoch [2/300], Loss: 0.0379\n",
      "Epoch [3/300], Loss: 0.0339\n",
      "Epoch [4/300], Loss: 0.0364\n",
      "Epoch [5/300], Loss: 0.0380\n",
      "Epoch [6/300], Loss: 0.0325\n",
      "Epoch [7/300], Loss: 0.0411\n",
      "Epoch [8/300], Loss: 0.0408\n",
      "Epoch [9/300], Loss: 0.0431\n",
      "Epoch [10/300], Loss: 0.0475\n",
      "Epoch [11/300], Loss: 0.0406\n",
      "Epoch [12/300], Loss: 0.0363\n",
      "Epoch [13/300], Loss: 0.0365\n",
      "Epoch [14/300], Loss: 0.0346\n",
      "Epoch [15/300], Loss: 0.0373\n",
      "Epoch [16/300], Loss: 0.0349\n",
      "Epoch [17/300], Loss: 0.0355\n",
      "Epoch [18/300], Loss: 0.0366\n",
      "Epoch [19/300], Loss: 0.0363\n",
      "Epoch [20/300], Loss: 0.0380\n",
      "Epoch [21/300], Loss: 0.0387\n",
      "Epoch [22/300], Loss: 0.0384\n",
      "Epoch [23/300], Loss: 0.0490\n",
      "Epoch [24/300], Loss: 0.0348\n",
      "Epoch [25/300], Loss: 0.0342\n",
      "Epoch [26/300], Loss: 0.0342\n",
      "Epoch [27/300], Loss: 0.0383\n",
      "Epoch [28/300], Loss: 0.0358\n",
      "Epoch [29/300], Loss: 0.0495\n",
      "Epoch [30/300], Loss: 0.0360\n",
      "Epoch [31/300], Loss: 0.0330\n",
      "Epoch [32/300], Loss: 0.0424\n",
      "Epoch [33/300], Loss: 0.0383\n",
      "Epoch [34/300], Loss: 0.0322\n",
      "Epoch [35/300], Loss: 0.0418\n",
      "Epoch [36/300], Loss: 0.0433\n",
      "Epoch [37/300], Loss: 0.0338\n",
      "Epoch [38/300], Loss: 0.0375\n",
      "Epoch [39/300], Loss: 0.0438\n",
      "Epoch [40/300], Loss: 0.0323\n",
      "Epoch [41/300], Loss: 0.0430\n",
      "Epoch [42/300], Loss: 0.0448\n",
      "Epoch [43/300], Loss: 0.0293\n",
      "Epoch [44/300], Loss: 0.0383\n",
      "Epoch [45/300], Loss: 0.0337\n",
      "Epoch [46/300], Loss: 0.0405\n",
      "Epoch [47/300], Loss: 0.0371\n",
      "Epoch [48/300], Loss: 0.0311\n",
      "Epoch [49/300], Loss: 0.0390\n",
      "Epoch [50/300], Loss: 0.0340\n",
      "Epoch [51/300], Loss: 0.0341\n",
      "Epoch [52/300], Loss: 0.0370\n",
      "Epoch [53/300], Loss: 0.0398\n",
      "Epoch [54/300], Loss: 0.0425\n",
      "Epoch [55/300], Loss: 0.0407\n",
      "Epoch [56/300], Loss: 0.0396\n",
      "Epoch [57/300], Loss: 0.0361\n",
      "Epoch [58/300], Loss: 0.0369\n",
      "Epoch [59/300], Loss: 0.0404\n",
      "Epoch [60/300], Loss: 0.0410\n",
      "Epoch [61/300], Loss: 0.0325\n",
      "Epoch [62/300], Loss: 0.0325\n",
      "Epoch [63/300], Loss: 0.0303\n",
      "Epoch [64/300], Loss: 0.0363\n",
      "Epoch [65/300], Loss: 0.0365\n",
      "Epoch [66/300], Loss: 0.0382\n",
      "Epoch [67/300], Loss: 0.0394\n",
      "Epoch [68/300], Loss: 0.0369\n",
      "Epoch [69/300], Loss: 0.0393\n",
      "Epoch [70/300], Loss: 0.0377\n",
      "Epoch [71/300], Loss: 0.0358\n",
      "Epoch [72/300], Loss: 0.0387\n",
      "Epoch [73/300], Loss: 0.0434\n",
      "Epoch [74/300], Loss: 0.0352\n",
      "Epoch [75/300], Loss: 0.0439\n",
      "Epoch [76/300], Loss: 0.0418\n",
      "Epoch [77/300], Loss: 0.0354\n",
      "Epoch [78/300], Loss: 0.0394\n",
      "Epoch [79/300], Loss: 0.0365\n",
      "Epoch [80/300], Loss: 0.0405\n",
      "Epoch [81/300], Loss: 0.0413\n",
      "Epoch [82/300], Loss: 0.0388\n",
      "Epoch [83/300], Loss: 0.0384\n",
      "Epoch [84/300], Loss: 0.0324\n",
      "Epoch [85/300], Loss: 0.0362\n",
      "Epoch [86/300], Loss: 0.0446\n",
      "Epoch [87/300], Loss: 0.0375\n",
      "Epoch [88/300], Loss: 0.0302\n",
      "Epoch [89/300], Loss: 0.0360\n",
      "Epoch [90/300], Loss: 0.0410\n",
      "Epoch [91/300], Loss: 0.0448\n",
      "Epoch [92/300], Loss: 0.0383\n",
      "Epoch [93/300], Loss: 0.0316\n",
      "Epoch [94/300], Loss: 0.0324\n",
      "Epoch [95/300], Loss: 0.0358\n",
      "Epoch [96/300], Loss: 0.0403\n",
      "Epoch [97/300], Loss: 0.0310\n",
      "Epoch [98/300], Loss: 0.0325\n",
      "Epoch [99/300], Loss: 0.0408\n",
      "Epoch [100/300], Loss: 0.0329\n",
      "Epoch [101/300], Loss: 0.0352\n",
      "Epoch [102/300], Loss: 0.0387\n",
      "Epoch [103/300], Loss: 0.0351\n",
      "Epoch [104/300], Loss: 0.0411\n",
      "Epoch [105/300], Loss: 0.0352\n",
      "Epoch [106/300], Loss: 0.0407\n",
      "Epoch [107/300], Loss: 0.0437\n",
      "Epoch [108/300], Loss: 0.0360\n",
      "Epoch [109/300], Loss: 0.0394\n",
      "Epoch [110/300], Loss: 0.0340\n",
      "Epoch [111/300], Loss: 0.0374\n",
      "Epoch [112/300], Loss: 0.0388\n",
      "Epoch [113/300], Loss: 0.0353\n",
      "Epoch [114/300], Loss: 0.0475\n",
      "Epoch [115/300], Loss: 0.0312\n",
      "Epoch [116/300], Loss: 0.0351\n",
      "Epoch [117/300], Loss: 0.0340\n",
      "Epoch [118/300], Loss: 0.0427\n",
      "Epoch [119/300], Loss: 0.0392\n",
      "Epoch [120/300], Loss: 0.0465\n",
      "Epoch [121/300], Loss: 0.0379\n",
      "Epoch [122/300], Loss: 0.0422\n",
      "Epoch [123/300], Loss: 0.0422\n",
      "Epoch [124/300], Loss: 0.0334\n",
      "Epoch [125/300], Loss: 0.0386\n",
      "Epoch [126/300], Loss: 0.0374\n",
      "Epoch [127/300], Loss: 0.0354\n",
      "Epoch [128/300], Loss: 0.0464\n",
      "Epoch [129/300], Loss: 0.0364\n",
      "Epoch [130/300], Loss: 0.0361\n",
      "Epoch [131/300], Loss: 0.0337\n",
      "Epoch [132/300], Loss: 0.0390\n",
      "Epoch [133/300], Loss: 0.0373\n",
      "Epoch [134/300], Loss: 0.0390\n",
      "Epoch [135/300], Loss: 0.0384\n",
      "Epoch [136/300], Loss: 0.0364\n",
      "Epoch [137/300], Loss: 0.0377\n",
      "Epoch [138/300], Loss: 0.0409\n",
      "Epoch [139/300], Loss: 0.0361\n",
      "Epoch [140/300], Loss: 0.0332\n",
      "Epoch [141/300], Loss: 0.0357\n",
      "Epoch [142/300], Loss: 0.0358\n",
      "Epoch [143/300], Loss: 0.0378\n",
      "Epoch [144/300], Loss: 0.0423\n",
      "Epoch [145/300], Loss: 0.0362\n",
      "Epoch [146/300], Loss: 0.0364\n",
      "Epoch [147/300], Loss: 0.0427\n",
      "Epoch [148/300], Loss: 0.0374\n",
      "Epoch [149/300], Loss: 0.0349\n",
      "Epoch [150/300], Loss: 0.0385\n",
      "Epoch [151/300], Loss: 0.0387\n",
      "Epoch [152/300], Loss: 0.0413\n",
      "Epoch [153/300], Loss: 0.0392\n",
      "Epoch [154/300], Loss: 0.0349\n",
      "Epoch [155/300], Loss: 0.0380\n",
      "Epoch [156/300], Loss: 0.0455\n",
      "Epoch [157/300], Loss: 0.0405\n",
      "Epoch [158/300], Loss: 0.0401\n",
      "Epoch [159/300], Loss: 0.0438\n",
      "Epoch [160/300], Loss: 0.0336\n",
      "Epoch [161/300], Loss: 0.0395\n",
      "Epoch [162/300], Loss: 0.0388\n",
      "Epoch [163/300], Loss: 0.0372\n",
      "Epoch [164/300], Loss: 0.0382\n",
      "Epoch [165/300], Loss: 0.0343\n",
      "Epoch [166/300], Loss: 0.0329\n",
      "Epoch [167/300], Loss: 0.0386\n",
      "Epoch [168/300], Loss: 0.0372\n",
      "Epoch [169/300], Loss: 0.0358\n",
      "Epoch [170/300], Loss: 0.0416\n",
      "Epoch [171/300], Loss: 0.0347\n",
      "Epoch [172/300], Loss: 0.0346\n",
      "Epoch [173/300], Loss: 0.0360\n",
      "Epoch [174/300], Loss: 0.0423\n",
      "Epoch [175/300], Loss: 0.0398\n",
      "Epoch [176/300], Loss: 0.0348\n",
      "Epoch [177/300], Loss: 0.0329\n",
      "Epoch [178/300], Loss: 0.0360\n",
      "Epoch [179/300], Loss: 0.0350\n",
      "Epoch [180/300], Loss: 0.0388\n",
      "Epoch [181/300], Loss: 0.0396\n",
      "Epoch [182/300], Loss: 0.0369\n",
      "Epoch [183/300], Loss: 0.0384\n",
      "Epoch [184/300], Loss: 0.0317\n",
      "Epoch [185/300], Loss: 0.0423\n",
      "Epoch [186/300], Loss: 0.0398\n",
      "Epoch [187/300], Loss: 0.0441\n",
      "Epoch [188/300], Loss: 0.0431\n",
      "Epoch [189/300], Loss: 0.0348\n",
      "Epoch [190/300], Loss: 0.0456\n",
      "Epoch [191/300], Loss: 0.0481\n",
      "Epoch [192/300], Loss: 0.0408\n",
      "Epoch [193/300], Loss: 0.0386\n",
      "Epoch [194/300], Loss: 0.0299\n",
      "Epoch [195/300], Loss: 0.0394\n",
      "Epoch [196/300], Loss: 0.0375\n",
      "Epoch [197/300], Loss: 0.0366\n",
      "Epoch [198/300], Loss: 0.0445\n",
      "Epoch [199/300], Loss: 0.0374\n",
      "Epoch [200/300], Loss: 0.0362\n",
      "Epoch [201/300], Loss: 0.0359\n",
      "Epoch [202/300], Loss: 0.0357\n",
      "Epoch [203/300], Loss: 0.0312\n",
      "Epoch [204/300], Loss: 0.0376\n",
      "Epoch [205/300], Loss: 0.0325\n",
      "Epoch [206/300], Loss: 0.0393\n",
      "Epoch [207/300], Loss: 0.0346\n",
      "Epoch [208/300], Loss: 0.0373\n",
      "Epoch [209/300], Loss: 0.0341\n",
      "Epoch [210/300], Loss: 0.0353\n",
      "Epoch [211/300], Loss: 0.0333\n",
      "Epoch [212/300], Loss: 0.0392\n",
      "Epoch [213/300], Loss: 0.0344\n",
      "Epoch [214/300], Loss: 0.0336\n",
      "Epoch [215/300], Loss: 0.0427\n",
      "Epoch [216/300], Loss: 0.0393\n",
      "Epoch [217/300], Loss: 0.0400\n",
      "Epoch [218/300], Loss: 0.0382\n",
      "Epoch [219/300], Loss: 0.0405\n",
      "Epoch [220/300], Loss: 0.0366\n",
      "Epoch [221/300], Loss: 0.0459\n",
      "Epoch [222/300], Loss: 0.0442\n",
      "Epoch [223/300], Loss: 0.0407\n",
      "Epoch [224/300], Loss: 0.0401\n",
      "Epoch [225/300], Loss: 0.0313\n",
      "Epoch [226/300], Loss: 0.0371\n",
      "Epoch [227/300], Loss: 0.0366\n",
      "Epoch [228/300], Loss: 0.0369\n",
      "Epoch [229/300], Loss: 0.0423\n",
      "Epoch [230/300], Loss: 0.0345\n",
      "Epoch [231/300], Loss: 0.0362\n",
      "Epoch [232/300], Loss: 0.0397\n",
      "Epoch [233/300], Loss: 0.0346\n",
      "Epoch [234/300], Loss: 0.0406\n",
      "Epoch [235/300], Loss: 0.0362\n",
      "Epoch [236/300], Loss: 0.0363\n",
      "Epoch [237/300], Loss: 0.0362\n",
      "Epoch [238/300], Loss: 0.0426\n",
      "Epoch [239/300], Loss: 0.0346\n",
      "Epoch [240/300], Loss: 0.0420\n",
      "Epoch [241/300], Loss: 0.0364\n",
      "Epoch [242/300], Loss: 0.0343\n",
      "Epoch [243/300], Loss: 0.0386\n",
      "Epoch [244/300], Loss: 0.0404\n",
      "Epoch [245/300], Loss: 0.0323\n",
      "Epoch [246/300], Loss: 0.0435\n",
      "Epoch [247/300], Loss: 0.0358\n",
      "Epoch [248/300], Loss: 0.0421\n",
      "Epoch [249/300], Loss: 0.0413\n",
      "Epoch [250/300], Loss: 0.0384\n",
      "Epoch [251/300], Loss: 0.0402\n",
      "Epoch [252/300], Loss: 0.0380\n",
      "Epoch [253/300], Loss: 0.0339\n",
      "Epoch [254/300], Loss: 0.0371\n",
      "Epoch [255/300], Loss: 0.0393\n",
      "Epoch [256/300], Loss: 0.0435\n",
      "Epoch [257/300], Loss: 0.0373\n",
      "Epoch [258/300], Loss: 0.0387\n",
      "Epoch [259/300], Loss: 0.0348\n",
      "Epoch [260/300], Loss: 0.0382\n",
      "Epoch [261/300], Loss: 0.0407\n",
      "Epoch [262/300], Loss: 0.0405\n",
      "Epoch [263/300], Loss: 0.0434\n",
      "Epoch [264/300], Loss: 0.0365\n",
      "Epoch [265/300], Loss: 0.0409\n",
      "Epoch [266/300], Loss: 0.0378\n",
      "Epoch [267/300], Loss: 0.0408\n",
      "Epoch [268/300], Loss: 0.0362\n",
      "Epoch [269/300], Loss: 0.0365\n",
      "Epoch [270/300], Loss: 0.0423\n",
      "Epoch [271/300], Loss: 0.0440\n",
      "Epoch [272/300], Loss: 0.0445\n",
      "Epoch [273/300], Loss: 0.0391\n",
      "Epoch [274/300], Loss: 0.0409\n",
      "Epoch [275/300], Loss: 0.0351\n",
      "Epoch [276/300], Loss: 0.0387\n",
      "Epoch [277/300], Loss: 0.0329\n",
      "Epoch [278/300], Loss: 0.0390\n",
      "Epoch [279/300], Loss: 0.0358\n",
      "Epoch [280/300], Loss: 0.0395\n",
      "Epoch [281/300], Loss: 0.0379\n",
      "Epoch [282/300], Loss: 0.0431\n",
      "Epoch [283/300], Loss: 0.0343\n",
      "Epoch [284/300], Loss: 0.0383\n",
      "Epoch [285/300], Loss: 0.0322\n",
      "Epoch [286/300], Loss: 0.0343\n",
      "Epoch [287/300], Loss: 0.0327\n",
      "Epoch [288/300], Loss: 0.0357\n",
      "Epoch [289/300], Loss: 0.0369\n",
      "Epoch [290/300], Loss: 0.0432\n",
      "Epoch [291/300], Loss: 0.0386\n",
      "Epoch [292/300], Loss: 0.0361\n",
      "Epoch [293/300], Loss: 0.0378\n",
      "Epoch [294/300], Loss: 0.0363\n",
      "Epoch [295/300], Loss: 0.0382\n",
      "Epoch [296/300], Loss: 0.0383\n",
      "Epoch [297/300], Loss: 0.0333\n",
      "Epoch [298/300], Loss: 0.0354\n",
      "Epoch [299/300], Loss: 0.0353\n",
      "Epoch [300/300], Loss: 0.0337\n",
      "average_loss: 0.0382\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "411b8088f0980def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-27T11:31:55.130792Z",
     "start_time": "2024-06-27T11:31:54.637225Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4149e438a8f03b81",
   "metadata": {},
   "source": [
    "y1 = output_scaler.inverse_transform(batch_targets.cpu())\n",
    "y2 = output_scaler.inverse_transform(outputs.cpu())\n",
    "\n",
    "for i in range(0, len(y1)):\n",
    "    plt.figure()\n",
    "    plt.plot(y1[i, :], color='red')\n",
    "    plt.plot(y2[i, :])\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a7670ce4509ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
